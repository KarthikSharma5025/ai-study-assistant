 # ğŸ“˜ AI Study Assistant

A lightweight AI-powered study assistant built using **Flask** and **Ollama (LLaMA 3)**.  
This application allows students to ask academic questions through a clean web interface and receive AI-generated responses.

---

## ğŸš€ Features

- ğŸ§  Local AI powered by LLaMA 3 (via Ollama)
- ğŸŒ Flask web interface
- ğŸ’¬ Interactive chat UI
- âš¡ Fast response generation
- ğŸ”’ Runs fully offline (when using local Ollama)
- ğŸŒ Can be deployed for global access

---

## ğŸ—ï¸ Project Structure

```
.
â”œâ”€â”€ app.py              # Flask backend server
â”œâ”€â”€ ai_agent.py         # AI agent logic (Ollama integration)
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html      # Frontend HTML
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ style.css       # Styling
â”‚   â””â”€â”€ script.js       # Frontend JavaScript
â””â”€â”€ README.md
```

---

## ğŸ§  How It Works

1. User submits a question via web interface.
2. Flask backend receives the request.
3. `ai_agent.py` sends the prompt to Ollama API.
4. LLaMA 3 generates response.
5. Response is returned to frontend and displayed.

---

## ğŸ”§ Requirements

- Python 3.9+
- Flask
- Requests
- Ollama installed locally
- LLaMA 3 model pulled in Ollama

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone Repository

```bash
https://github.com/KarthikSharma5025/ai-study-assistant.git
cd ai-study-assistant
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install flask requests
```

### 3ï¸âƒ£ Install Ollama

Download from:

https://ollama.com/

Then pull the model:

```bash
ollama pull llama3
```

---

## â–¶ï¸ Running the Application (Local)

```bash
python app.py
```

Open:

```
http://127.0.0.1:5000
```

---

## ğŸŒ Running for Network Access

Modify `app.py`:

```python
app.run(host="0.0.0.0", port=5000)
```

Then other devices on same WiFi can access:

```
http://YOUR-IP-ADDRESS:5000
```

---

## ğŸŒ Deploying Globally

To make this accessible worldwide:

- Deploy to a VPS (AWS / DigitalOcean / Render)
- OR use tunneling tools like ngrok
- OR replace Ollama with OpenAI API and deploy to cloud

âš ï¸ Note: If using Ollama locally, your machine must remain online.

---

## ğŸ” Security Recommendations

- Disable `debug=True` in production
- Add authentication system
- Add rate limiting
- Use HTTPS in production
- Never expose local development server publicly without firewall rules

---

## ğŸ¯ Future Improvements

- User login system
- Chat history storage
- Multi-agent support
- File upload support (PDF notes)
- Docker containerization
- Production WSGI server (Gunicorn)

---

## ğŸ“œ License

This project is for educational purposes.

---

## ğŸ‘¨â€ğŸ’» Author

Built as a custom AI study tool using Flask and local LLM integration.
